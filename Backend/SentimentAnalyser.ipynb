{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZSFj7a1WJHi2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from string import punctuation\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LXr9IXceJe53"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QxCsbj8lJlMw"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train.csv',encoding='latin1');\n",
        "test_data = pd.read_csv('test.csv',encoding='latin1');\n",
        "df = pd.concat([train_data,test_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "ZjQm4NnALQQn",
        "outputId": "3d30f2d9-5adb-4169-aebd-dbe18efb1096"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Time of Tweet</th>\n",
              "      <th>Age of User</th>\n",
              "      <th>Country</th>\n",
              "      <th>Population -2020</th>\n",
              "      <th>Land Area (Km²)</th>\n",
              "      <th>Density (P/Km²)</th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "      <td>morning</td>\n",
              "      <td>0-20</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>38928346.0</td>\n",
              "      <td>652860.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "      <td>noon</td>\n",
              "      <td>21-30</td>\n",
              "      <td>Albania</td>\n",
              "      <td>2877797.0</td>\n",
              "      <td>27400.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "      <td>night</td>\n",
              "      <td>31-45</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>43851044.0</td>\n",
              "      <td>2381740.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "      <td>morning</td>\n",
              "      <td>46-60</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>77265.0</td>\n",
              "      <td>470.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "      <td>noon</td>\n",
              "      <td>60-70</td>\n",
              "      <td>Angola</td>\n",
              "      <td>32866272.0</td>\n",
              "      <td>1246700.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID                                               text  \\\n",
              "0  cb774db0d1                I`d have responded, if I were going   \n",
              "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
              "2  088c60f138                          my boss is bullying me...   \n",
              "3  9642c003ef                     what interview! leave me alone   \n",
              "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
              "\n",
              "                         selected_text sentiment Time of Tweet Age of User  \\\n",
              "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
              "1                             Sooo SAD  negative          noon       21-30   \n",
              "2                          bullying me  negative         night       31-45   \n",
              "3                       leave me alone  negative       morning       46-60   \n",
              "4                        Sons of ****,  negative          noon       60-70   \n",
              "\n",
              "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  id tweet  \n",
              "0  Afghanistan        38928346.0         652860.0             60.0 NaN   NaN  \n",
              "1      Albania         2877797.0          27400.0            105.0 NaN   NaN  \n",
              "2      Algeria        43851044.0        2381740.0             18.0 NaN   NaN  \n",
              "3      Andorra           77265.0            470.0            164.0 NaN   NaN  \n",
              "4       Angola        32866272.0        1246700.0             26.0 NaN   NaN  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ppg5Q16ULSe4"
      },
      "outputs": [],
      "source": [
        "def remove_unnecessary_characters(text):\n",
        "    text = re.sub(r'<.*?>', '', str(text))\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', str(text))\n",
        "    text = re.sub(r'\\s+', ' ', str(text)).strip()\n",
        "    return text\n",
        "df['clean_text'] = df['text'].apply(remove_unnecessary_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5Q0iaZALYPf",
        "outputId": "30a6fdee-0c3a-443e-cbc3-c3f6095740b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "def tokenize_text(text):\n",
        "    try:\n",
        "        text = str(text)\n",
        "        tokens = word_tokenize(text)\n",
        "        return tokens\n",
        "    except Exception as e:\n",
        "        print(f\"Error tokenizing text: {e}\")\n",
        "        return []\n",
        "df['tokens'] = df['text'].apply(tokenize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iqs0BBLHLtpg"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    else:\n",
        "        text = str(text)\n",
        "    return text\n",
        "df['normalized_text'] = df['text'].apply(normalize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGr_y1uULwrv",
        "outputId": "40a40b9a-a528-411d-d554-eb22f037db17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "        filtered_text = ' '.join(filtered_words)\n",
        "    else:\n",
        "        filtered_text = ''\n",
        "    return filtered_text\n",
        "df['text_without_stopwords'] = df['text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EBEF07EuL4T_"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cPXbV4HeMOxp"
      },
      "outputs": [],
      "source": [
        "df['sentiment_code'] = df['sentiment'].astype('category').cat.codes\n",
        "sentiment_distribution = df['sentiment_code'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTSzNxVRMaCo",
        "outputId": "709881f6-bf9d-4544-8817-8ac81b67358d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame shape: (0, 17)\n",
            "Is DataFrame empty? True\n",
            "Columns: Index(['textID', 'text', 'selected_text', 'sentiment', 'Time of Tweet',\n",
            "       'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)',\n",
            "       'Density (P/Km²)', 'id', 'tweet', 'clean_text', 'tokens',\n",
            "       'normalized_text', 'text_without_stopwords', 'sentiment_code'],\n",
            "      dtype='object')\n",
            "Missing values in 'text': 0\n",
            "No valid 'text' column found.\n"
          ]
        }
      ],
      "source": [
        "print(\"DataFrame shape:\", df.shape)\n",
        "print(\"Is DataFrame empty?\", df.empty)\n",
        "print(\"Columns:\", df.columns)\n",
        "print(\"Missing values in 'text':\", df['text'].isna().sum())\n",
        "\n",
        "if 'text' in df.columns and not df.empty:\n",
        "    corpus = df['text'].dropna().tolist()\n",
        "    print(\"Corpus length:\", len(corpus))\n",
        "    if corpus:  # Check if list is not empty\n",
        "        print(\"First text sample:\", corpus[0])\n",
        "    else:\n",
        "        print(\"Corpus is empty after removing NaN values.\")\n",
        "else:\n",
        "    print(\"No valid 'text' column found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "vEwBtEwEMjsj",
        "outputId": "43bd480d-b5b2-4162-b017-12e6fc41f092"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, sentiment]\n",
              "Index: []"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_corpus = df['text'].astype(str).tolist()\n",
        "data_eda = pd.DataFrame()\n",
        "data_eda['text'] = final_corpus\n",
        "data_eda['sentiment'] = df[\"sentiment\"].values\n",
        "data_eda.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JgQ2Xe2nMlg3"
      },
      "outputs": [],
      "source": [
        "# df['Time of Tweet'] = df['Time of Tweet'].astype('category').cat.codes\n",
        "# df['Country'] = df['Country'].astype('category').cat.codes\n",
        "# df['Age of User']=df['Age of User'].replace({'0-20':18,'21-30':25,'31-45':38,'46-60':53,'60-70':65,'70-100':80})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0coULwYgMpaH"
      },
      "outputs": [],
      "source": [
        "df=df.drop(columns=['textID','Time of Tweet', 'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Aq3c7N5ONI5v"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def wp(text):\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "# Assuming df is defined somewhere in your code\n",
        "df['selected_text'] = df[\"selected_text\"].apply(wp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vQohR5GbNMdX"
      },
      "outputs": [],
      "source": [
        "X=df['selected_text']\n",
        "y= df['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh_sHkZINP_5",
        "outputId": "25420f83-d8f5-466d-975e-be56d6eb53ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UTF-8 decoding failed. Trying 'latin1' encoding...\n",
            "Dataset shape: (27481, 10)\n",
            "Column names: Index(['textID', 'text', 'selected_text', 'sentiment', 'Time of Tweet',\n",
            "       'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)',\n",
            "       'Density (P/Km²)'],\n",
            "      dtype='object')\n",
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment Time of Tweet Age of User  \\\n",
            "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
            "1                             Sooo SAD  negative          noon       21-30   \n",
            "2                          bullying me  negative         night       31-45   \n",
            "3                       leave me alone  negative       morning       46-60   \n",
            "4                        Sons of ****,  negative          noon       60-70   \n",
            "\n",
            "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
            "0  Afghanistan          38928346         652860.0               60  \n",
            "1      Albania           2877797          27400.0              105  \n",
            "2      Algeria          43851044        2381740.0               18  \n",
            "3      Andorra             77265            470.0              164  \n",
            "4       Angola          32866272        1246700.0               26  \n",
            "Missing values in the dataset:\n",
            "textID              0\n",
            "text                1\n",
            "selected_text       1\n",
            "sentiment           0\n",
            "Time of Tweet       0\n",
            "Age of User         0\n",
            "Country             0\n",
            "Population -2020    0\n",
            "Land Area (Km²)     0\n",
            "Density (P/Km²)     0\n",
            "dtype: int64\n",
            "Cleaned dataset shape: (27480, 10)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset with a specific encoding\n",
        "try:\n",
        "    df = pd.read_csv('train.csv', encoding='utf-8')  # Try UTF-8 first\n",
        "except UnicodeDecodeError:\n",
        "    print(\"UTF-8 decoding failed. Trying 'latin1' encoding...\")\n",
        "    df = pd.read_csv('train.csv', encoding='latin1')  # Fallback to latin1\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Column names:\", df.columns)\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in the dataset:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing values if necessary\n",
        "df = df.dropna()\n",
        "\n",
        "# Verify the cleaned dataset\n",
        "print(\"Cleaned dataset shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mV2-TSanNcn9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHu44FqoObL2",
        "outputId": "0092c947-b1e8-450d-c8a6-7d351dbf6ba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UTF-8 decoding failed. Trying 'latin1' encoding...\n",
            "Dataset shape: (27481, 10)\n",
            "Column names: Index(['textID', 'text', 'selected_text', 'sentiment', 'Time of Tweet',\n",
            "       'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)',\n",
            "       'Density (P/Km²)'],\n",
            "      dtype='object')\n",
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment Time of Tweet Age of User  \\\n",
            "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
            "1                             Sooo SAD  negative          noon       21-30   \n",
            "2                          bullying me  negative         night       31-45   \n",
            "3                       leave me alone  negative       morning       46-60   \n",
            "4                        Sons of ****,  negative          noon       60-70   \n",
            "\n",
            "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
            "0  Afghanistan          38928346         652860.0               60  \n",
            "1      Albania           2877797          27400.0              105  \n",
            "2      Algeria          43851044        2381740.0               18  \n",
            "3      Andorra             77265            470.0              164  \n",
            "4       Angola          32866272        1246700.0               26  \n",
            "Accuracy: 0.64\n",
            "Precision: 0.69\n",
            "Recall: 0.64\n",
            "F1 Score: 0.63\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv('train.csv', encoding='utf-8')  # Try UTF-8 first\n",
        "except UnicodeDecodeError:\n",
        "    print(\"UTF-8 decoding failed. Trying 'latin1' encoding...\")\n",
        "    df = pd.read_csv('train.csv', encoding='latin1')  # Fallback to latin1\n",
        "\n",
        "# Step 2: Inspect the dataset\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Column names:\", df.columns)\n",
        "print(df.head())\n",
        "\n",
        "# Step 3: Preprocess the text data\n",
        "def remove_unnecessary_characters(text):\n",
        "    if pd.isna(text):  # Handle NaN or None values\n",
        "        return ''\n",
        "    text = re.sub(r'<.*?>', '', str(text))  # Remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', str(text))  # Keep only alphanumeric and spaces\n",
        "    text = re.sub(r'\\s+', ' ', str(text)).strip()  # Replace multiple spaces with a single space and trim\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(remove_unnecessary_characters)\n",
        "\n",
        "# Step 4: Define X (features) and y (target)\n",
        "X = df['clean_text']  # Use the cleaned text as features\n",
        "y = df['sentiment']   # Use the 'sentiment' column as the target\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Text Vectorization with trigram support\n",
        "vectorizer = TfidfVectorizer(max_features=1500, ngram_range=(1, 3))\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 7: Define the Random Forest classifier with expanded hyperparameter search\n",
        "param_grid = {'n_estimators': [200, 400, 600], 'max_depth': [15, 25, 35]}\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(rf_classifier, param_grid, cv=3, n_jobs=-1)\n",
        "grid_search.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set using the best estimator from grid search\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "y_pred = best_rf_classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Step 9: Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmLH1Yi9QDcn",
        "outputId": "263a10f6-2f7e-4011-8934-1d26a4e20b32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sentiment_model.pkl']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(best_rf_classifier, 'sentiment_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2RtjcHrQJQx",
        "outputId": "6be445f7-b22c-46ad-b339-429fde7412cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Sentiment: neutral\n"
          ]
        }
      ],
      "source": [
        "loaded_model = joblib.load('sentiment_model.pkl')\n",
        "\n",
        "new_text = \"I hate you\"\n",
        "\n",
        "new_text_vectorized = vectorizer.transform([new_text])\n",
        "\n",
        "predicted_sentiment = loaded_model.predict(new_text_vectorized)\n",
        "\n",
        "print(f'Predicted Sentiment: {predicted_sentiment[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "vcDaD7wtal58",
        "outputId": "9bb47114-a5bf-4b90-c430-c2da674cadd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UTF-8 decoding failed for train.csv. Trying 'latin1' encoding...\n",
            "Training dataset shape: (27481, 10)\n",
            "Testing dataset shape: (17197, 2)\n",
            "Training dataset columns: Index(['textID', 'text', 'selected_text', 'sentiment', 'Time of Tweet',\n",
            "       'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)',\n",
            "       'Density (P/Km²)'],\n",
            "      dtype='object')\n",
            "Testing dataset columns: Index(['id', 'tweet'], dtype='object')\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'text'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Web Development\\sentiment\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Apply preprocessing to both training and testing datasets\u001b[39;00m\n\u001b[32m     37\u001b[39m train_data[\u001b[33m'\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m'\u001b[39m] = train_data[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].apply(remove_unnecessary_characters)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m test_data[\u001b[33m'\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(remove_unnecessary_characters)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Step 4: Define features (X) and target (y) for training\u001b[39;00m\n\u001b[32m     41\u001b[39m X_train = train_data[\u001b[33m'\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Web Development\\sentiment\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Web Development\\sentiment\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'text'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the datasets\n",
        "try:\n",
        "    train_data = pd.read_csv('train.csv', encoding='utf-8')  # Try UTF-8 first\n",
        "except UnicodeDecodeError:\n",
        "    print(\"UTF-8 decoding failed for train.csv. Trying 'latin1' encoding...\")\n",
        "    train_data = pd.read_csv('train.csv', encoding='latin1')  # Fallback to latin1\n",
        "\n",
        "try:\n",
        "    test_data = pd.read_csv('test.csv', encoding='utf-8')  # Try UTF-8 first\n",
        "except UnicodeDecodeError:\n",
        "    print(\"UTF-8 decoding failed for test.csv. Trying 'latin1' encoding...\")\n",
        "    test_data = pd.read_csv('test.csv', encoding='latin1')  # Fallback to latin1\n",
        "\n",
        "# Step 2: Inspect the datasets\n",
        "print(\"Training dataset shape:\", train_data.shape)\n",
        "print(\"Testing dataset shape:\", test_data.shape)\n",
        "print(\"Training dataset columns:\", train_data.columns)\n",
        "print(\"Testing dataset columns:\", test_data.columns)\n",
        "\n",
        "# Step 3: Preprocess the text data\n",
        "def remove_unnecessary_characters(text):\n",
        "    if pd.isna(text):  # Handle NaN or None values\n",
        "        return ''\n",
        "    text = re.sub(r'<.*?>', '', str(text))  # Remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', str(text))  # Keep only alphanumeric and spaces\n",
        "    text = re.sub(r'\\s+', ' ', str(text)).strip()  # Replace multiple spaces with a single space and trim\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to both training and testing datasets\n",
        "train_data['clean_text'] = train_data['text'].apply(remove_unnecessary_characters)\n",
        "test_data['clean_text'] = test_data['text'].apply(remove_unnecessary_characters)\n",
        "\n",
        "# Step 4: Define features (X) and target (y) for training\n",
        "X_train = train_data['clean_text']\n",
        "y_train = train_data['sentiment']\n",
        "\n",
        "# For testing, we only have features (no target labels in test.csv)\n",
        "X_test = test_data['clean_text']\n",
        "\n",
        "# Step 5: Vectorize the text data using TF-IDF with trigram support\n",
        "vectorizer = TfidfVectorizer(max_features=1500, ngram_range=(1, 3))\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Split the training data into training and validation sets\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train_vectorized, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 7: Train a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Step 8: Evaluate the model on the validation set\n",
        "y_val_pred = rf_classifier.predict(X_val)\n",
        "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation Accuracy: {validation_accuracy:.2f}')\n",
        "\n",
        "# Step 9: Make predictions on the test set\n",
        "test_predictions = rf_classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Step 10: Save the predictions to a CSV file (if needed)\n",
        "test_data['predicted_sentiment'] = test_predictions\n",
        "test_data[['textID', 'predicted_sentiment']].to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Test set predictions saved to 'submission.csv'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
